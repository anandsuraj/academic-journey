{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Financial Sentiment Analysis Assignment"
      ],
      "metadata": {
        "id": "uoOxqAMNlmln"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import required libraries"
      ],
      "metadata": {
        "id": "muyzFgm1lrf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy matplotlib seaborn nltk wordcloud scikit-learn gensim\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from gensim.models import Word2Vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "U_0Brejslv1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download required NLTK data"
      ],
      "metadata": {
        "id": "3UfqMybPl5O0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('brown')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "qJV3Geval8Dx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load dataset and basic exploration"
      ],
      "metadata": {
        "id": "EUTz6S3kmB02"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_explore_data():\n",
        "    # Load the dataset\n",
        "    fsa = pd.read_csv('/content/FinancialSentimentAnalysis.csv')\n",
        "\n",
        "    print(\"Dataset Head:\")\n",
        "    print(fsa.head())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Dataset Info:\")\n",
        "    print(fsa.info())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    print(\"Dataset Description:\")\n",
        "    print(fsa.describe())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    return fsa"
      ],
      "metadata": {
        "id": "0tvjPqRlmFy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Text preprocessing function"
      ],
      "metadata": {
        "id": "ED6u2ts7mKwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove punctuations, numbers, and special characters\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "XdWYe7MVmMfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Remove stopwords and normalize using lemmatization\n"
      ],
      "metadata": {
        "id": "ngomDx_emOzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_and_normalize(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)"
      ],
      "metadata": {
        "id": "Iw_CkCJTmQnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Create word cloud"
      ],
      "metadata": {
        "id": "bvMmbkatmTGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_wordcloud(text_data):\n",
        "    # Combine all text\n",
        "    all_text = ' '.join(text_data)\n",
        "\n",
        "    # Generate word cloud\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
        "\n",
        "    # Plot word cloud\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud of Cleaned Sentences')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bfStDBS7mUs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5A. Skip-gram model implementation"
      ],
      "metadata": {
        "id": "fatX5JccmXFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_skipgram_features(sentences):\n",
        "    # Tokenize sentences for Word2Vec\n",
        "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "    # Train Skip-gram model\n",
        "    skipgram_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=1, workers=4)  # sg=1 for skip-gram\n",
        "\n",
        "    # Create feature vectors by averaging word vectors\n",
        "    def get_sentence_vector(sentence):\n",
        "        words = sentence.split()\n",
        "        word_vectors = [skipgram_model.wv[word] for word in words if word in skipgram_model.wv]\n",
        "        if word_vectors:\n",
        "            return np.mean(word_vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(100)\n",
        "\n",
        "    feature_vectors = np.array([get_sentence_vector(sentence) for sentence in sentences])\n",
        "    return feature_vectors"
      ],
      "metadata": {
        "id": "VX0N0ErdmcMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5B. CBOW((Continuous Bag of Words)) model implementation\n"
      ],
      "metadata": {
        "id": "nxvZII_Ymd97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_cbow_features(sentences):\n",
        "    # Tokenize sentences for Word2Vec\n",
        "    tokenized_sentences = [sentence.split() for sentence in sentences]\n",
        "\n",
        "    # Train CBOW model\n",
        "    cbow_model = Word2Vec(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, sg=0, workers=4)  # sg=0 for CBOW\n",
        "\n",
        "    # Create feature vectors by averaging word vectors\n",
        "    def get_sentence_vector(sentence):\n",
        "        words = sentence.split()\n",
        "        word_vectors = [cbow_model.wv[word] for word in words if word in cbow_model.wv]\n",
        "        if word_vectors:\n",
        "            return np.mean(word_vectors, axis=0)\n",
        "        else:\n",
        "            return np.zeros(100)\n",
        "\n",
        "    feature_vectors = np.array([get_sentence_vector(sentence) for sentence in sentences])\n",
        "    return feature_vectors\n",
        "\n",
        "# Function to train decision tree and display results\n",
        "def train_and_evaluate(X_train, X_test, y_train, y_test, model_name):\n",
        "    # Train Decision Tree\n",
        "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
        "    dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "    # Predictions\n",
        "    train_pred = dt_classifier.predict(X_train)\n",
        "    test_pred = dt_classifier.predict(X_test)\n",
        "\n",
        "    # Confusion matrices\n",
        "    train_cm = confusion_matrix(y_train, train_pred)\n",
        "    test_cm = confusion_matrix(y_test, test_pred)\n",
        "\n",
        "    # Display results\n",
        "    print(f\"\\n{model_name} Results:\")\n",
        "    print(f\"Training Accuracy: {accuracy_score(y_train, train_pred):.4f}\")\n",
        "    print(f\"Testing Accuracy: {accuracy_score(y_test, test_pred):.4f}\")\n",
        "\n",
        "    # Plot confusion matrices\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    sns.heatmap(train_cm, annot=True, fmt='d', ax=axes[0], cmap='Blues')\n",
        "    axes[0].set_title(f'{model_name} - Training Confusion Matrix')\n",
        "    axes[0].set_xlabel('Predicted')\n",
        "    axes[0].set_ylabel('Actual')\n",
        "\n",
        "    sns.heatmap(test_cm, annot=True, fmt='d', ax=axes[1], cmap='Blues')\n",
        "    axes[1].set_title(f'{model_name} - Testing Confusion Matrix')\n",
        "    axes[1].set_xlabel('Predicted')\n",
        "    axes[1].set_ylabel('Actual')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return accuracy_score(y_test, test_pred)"
      ],
      "metadata": {
        "id": "Lr7nS10mmont"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. HMM POS Tagging function"
      ],
      "metadata": {
        "id": "faUhE8Bwmqzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hmm_pos_tagging(sentence):\n",
        "    # Tokenize the sentence\n",
        "    tokens = word_tokenize(sentence)\n",
        "\n",
        "    # POS tagging using NLTK (which uses HMM-based tagger)\n",
        "    pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "    print(\"HMM POS Tagging for first cleaned sentence:\")\n",
        "    print(\"Word\\t\\tPOS Tag\")\n",
        "    print(\"-\" * 30)\n",
        "    for word, pos in pos_tags:\n",
        "        print(f\"{word}\\t\\t{pos}\")\n",
        "\n",
        "    return pos_tags"
      ],
      "metadata": {
        "id": "IALx0rXtmtIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main execution"
      ],
      "metadata": {
        "id": "0edwIdAOmvPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    # 1. Load and explore data\n",
        "    print(\"Step 1: Loading and exploring dataset...\")\n",
        "    fsa = load_and_explore_data()\n",
        "\n",
        "    # 2 & 3. Preprocessing and normalization\n",
        "    print(\"Step 2 & 3: Preprocessing and normalizing text...\")\n",
        "    fsa['cleaned_sentence'] = fsa['Sentence'].apply(preprocess_text)\n",
        "    fsa['cleaned_sentence'] = fsa['cleaned_sentence'].apply(clean_and_normalize)\n",
        "\n",
        "    print(\"Sample cleaned sentences:\")\n",
        "    print(fsa[['Sentence', 'cleaned_sentence']].head())\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    # 4. Create word cloud\n",
        "    print(\"Step 4: Creating word cloud...\")\n",
        "    create_wordcloud(fsa['cleaned_sentence'])\n",
        "\n",
        "    # 5. Create X and Y objects\n",
        "    print(\"Step 5: Creating feature vectors and training models...\")\n",
        "    X_text = fsa['cleaned_sentence']\n",
        "    Y = fsa['Sentiment']\n",
        "\n",
        "    # 5A. Skip-gram model\n",
        "    print(\"5A. Skip-gram Model:\")\n",
        "    X_skipgram = create_skipgram_features(X_text)\n",
        "    X_train_sg, X_test_sg, y_train_sg, y_test_sg = train_test_split(\n",
        "        X_skipgram, Y, test_size=0.2, random_state=42, stratify=Y)\n",
        "\n",
        "    skipgram_accuracy = train_and_evaluate(X_train_sg, X_test_sg, y_train_sg, y_test_sg, \"Skip-gram\")\n",
        "\n",
        "    # 5B. CBOW model\n",
        "    print(\"5B. CBOW Model:\")\n",
        "    X_cbow = create_cbow_features(X_text)\n",
        "    X_train_cbow, X_test_cbow, y_train_cbow, y_test_cbow = train_test_split(\n",
        "        X_cbow, Y, test_size=0.2, random_state=42, stratify=Y)\n",
        "\n",
        "    cbow_accuracy = train_and_evaluate(X_train_cbow, X_test_cbow, y_train_cbow, y_test_cbow, \"CBOW\")\n",
        "\n",
        "    # 5C. Compare Skip-gram and CBOW\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"5C. Comparison between Skip-gram and CBOW:\")\n",
        "    print(f\"Skip-gram Test Accuracy: {skipgram_accuracy:.4f}\")\n",
        "    print(f\"CBOW Test Accuracy: {cbow_accuracy:.4f}\")\n",
        "\n",
        "    if skipgram_accuracy > cbow_accuracy:\n",
        "        print(\"\\nSkip-gram performs better than CBOW.\")\n",
        "        print(\"Skip-gram is better at capturing semantic relationships and works well with rare words.\")\n",
        "        print(\"It predicts context words from target word, making it effective for word similarity tasks.\")\n",
        "    else:\n",
        "        print(\"\\nCBOW performs better than Skip-gram.\")\n",
        "        print(\"CBOW is faster to train and works well with frequent words.\")\n",
        "        print(\"It predicts target word from context, making it effective for syntactic relationships.\")\n",
        "\n",
        "    # 6. HMM POS Tagging\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Step 6: HMM POS Tagging on first cleaned sentence:\")\n",
        "    first_cleaned_sentence = fsa['cleaned_sentence'].iloc[0]\n",
        "    hmm_pos_tagging(first_cleaned_sentence)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "e-6BgEbPmyGg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}